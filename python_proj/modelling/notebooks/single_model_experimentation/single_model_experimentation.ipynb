{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forces the notebook to always reload packages.\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single model experimentation\n",
    "\n",
    "One of the critical points of feedback on our process is that we use multiple models.\n",
    "We use these models to clearly distinguish between the types of variables we're testing: ecosystem contributions (in general, and in (non-)dependent projects) and collaboration metrics.\n",
    "\n",
    "In part, this was done because variables correlated, thus affecting the model's quality.\n",
    "This notebook explores how this correlation can be resolved.\n",
    "\n",
    "Looking at the results of [the revised ecosystem model](./python_proj/modelling/notebooks/single_model_experimentation/ecosystem_model.ipynb), it shows that `ln(1 + WeightedEcosystemSecondOrderDegreeCentrality)` and `ln(1 + EcosystemExperienceSubmitterPullRequestSubmissionCount)` linearly correlate with a Spearman coefficient $\\rho$ of $0.748$, which is substantial.\n",
    "\n",
    "**Goal 1:** The problem that we try to solve in this notebook is to remove this correlation without losing insights.\n",
    "\n",
    "**Goal 2:** A secondary objective here is to make the metric fully undirectional (w.r.t. directed vs. undirected graphs) as the original metric considered directed $\\lambda$-edges and undirected $\\mu$-edges.\n",
    "This likely has no impact of the results as they in- and out-degree correlates strongly, however, optimally, both of these should be undirected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized second-order degree centrality\n",
    "\n",
    "**Basic definitions:**\n",
    "- $x, y \\in V$ vertices/nodes in the graph.\n",
    "- $p, q \\in P$ projects in the ecosystem.\n",
    "- $\\langle x, y, p \\rangle_\\lambda$ a $\\lambda$-edge connecting nodes $x, y$ in project $p$.\n",
    "- $E_\\lambda(\\cdot)$ a set of $\\lambda$-edges with some number of constraints applied to them (e.g., who the edge connects to).\n",
    "\n",
    "**Normalized ecosystem second-order multi-layer degree centrality (i.e., node centrality):**\n",
    "\n",
    "We define node centrality of contributor $x$ in focal project $p$ as:\n",
    "$$S(x, p) = \\sum_{\\lambda, \\mu}^{L^2}w_\\lambda w_\\mu \\cdot d(x,p,\\lambda,\\mu)$$\n",
    "which multiplies the second-order degree centrality of a single pair of layers by multiplying them by their respective weights.\n",
    "\n",
    "Here, weights are defined as:\n",
    "$$w_\\lambda = \\frac{|E_\\lambda|}{\\sum_{\\mu \\not = \\lambda}^L |E_\\mu|}$$\n",
    "\n",
    "The second-order degree centrality of a single pair of layers is defined as:\n",
    "$$d(x, p, \\lambda, \\mu) = \\frac{1}{|R_\\lambda(x, p)|} \\sum^{R_\\lambda(x, p)}_{\\langle x, y, q \\rangle_\\lambda} \\big(|E_\\mu(y)| - |E_\\mu(y, x)|\\big)$$\n",
    "such that the first-order degree of all neighbors of $x$ are summed, minus the edges that connect $x$ and $y$.\n",
    "This sum is then normalized by the number of ecosystem-spanning $\\lambda$-edges that $x$ has (i.e., $|R_\\lambda(x, p)|$); theoretically, this should remove correlation between the degree of $x$ and their second-order degree centrality.\n",
    "\n",
    "The ecosystem-spannnig edges of $x$ are then defined as follows:\n",
    "$$R_\\lambda(x, p) = \\big\\{ \\langle x, y, q \\rangle_\\lambda ~|~ p \\not = q \\land \\langle x, y, q \\rangle_\\lambda \\in E_\\lambda(x) \\big\\}$$\n",
    "Here, $p$ and $q$ are project identifiers and are used to exclusively include edges that are not part of the focal project; i.e., they are part of the ecosystem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "Defines some experimentation constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from wmutils.file import safe_makedirs\n",
    "\n",
    "thread_count = -1\n",
    "window_size_in_days = 90\n",
    "\n",
    "output_path = Path('./data/libraries/output/npm-libraries-1.6.0-2020-01-12/non_ftc_data.csv').absolute()\n",
    "safe_makedirs(output_path.parent)\n",
    "output_path = str(output_path)\n",
    "\n",
    "chunk_base_path = str(Path('./data/tmp/data_chunks').absolute()) + \"/\"\n",
    "chunk_output_base_path = str(\n",
    "    Path(\"./data/tmp/sub_results\").absolute()) + \"/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data sampling\n",
    "\n",
    "We can't use pre-calculated data for this project as it lacks details to calculate $R_\\lambda$.\n",
    "Because the sliding window algorithm takes a long time to compute, we sample the last time window from the datasets.\n",
    "The last time window should have a sufficient number of data points to do this analysis with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Iterator\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "import python_proj.utils.exp_utils as exp_utils\n",
    "from python_proj.utils.exp_utils import iterate_through_multiple_chronological_issue_pr_datasets\n",
    "\n",
    "\n",
    "exp_utils.load_paths_for_eco()\n",
    "\n",
    "# Constants\n",
    "issue_dataset = \"issues_sorted_started_26_05_23_min_5_prs_no_invalid_no_dupes\"\n",
    "pr_dataset = \"pulls_sorted_started_26_05_23_min_5_prs_no_invalid_no_dupes\"\n",
    "\n",
    "# Sample window start.\n",
    "# Copied by running `tail -1` on the pr data file.\n",
    "last_pr_date = '2020-01-11T23:44:58Z'\n",
    "last_date = datetime.strptime(last_pr_date, exp_utils.DATETIME_FORMAT)\n",
    "window_start = last_date - timedelta(days=window_size_in_days)\n",
    "\n",
    "\n",
    "def create_file_for_final_window(output_path: Path, data_iter: Iterator[Dict]):\n",
    "    \"\"\"Creates a dataset for entries within the final time window.\"\"\"\n",
    "    with open(output_path, 'w+', encoding='utf-8') as output_file:\n",
    "        KEY = 'closed_at'\n",
    "        for entry in data_iter:\n",
    "            # ensures we don't get a missing key exception\n",
    "            if KEY not in entry:\n",
    "                continue\n",
    "\n",
    "            # ignores everything before the time window starts.\n",
    "            closed_at = datetime.strptime(\n",
    "                entry[KEY], exp_utils.DATETIME_FORMAT)\n",
    "            if closed_at < window_start:\n",
    "                continue\n",
    "\n",
    "            jdata = json.dumps(entry)\n",
    "            output_file.write(f'{jdata}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates data file for issues.\n",
    "# creates output file for prs\n",
    "input_issue_dataset_name = 'issues_last_window'\n",
    "input_pr_dataset = 'prs_last_window'\n",
    "\n",
    "issue_output_path = Path(exp_utils.CHRONOLOGICAL_DATASET_PATH(\n",
    "    file_name=input_issue_dataset_name, data_type='issues')).absolute()\n",
    "issues_iter = iterate_through_multiple_chronological_issue_pr_datasets([\n",
    "                                                                       issue_dataset], [])\n",
    "\n",
    "\n",
    "pr_output_path = Path(exp_utils.CHRONOLOGICAL_DATASET_PATH(\n",
    "    file_name=input_pr_dataset, data_type='pull-requests')).absolute()\n",
    "pr_iter = iterate_through_multiple_chronological_issue_pr_datasets([], [\n",
    "                                                                   pr_dataset])\n",
    "\n",
    "# This step can be skipped.\n",
    "sample_last_month = False\n",
    "if sample_last_month:\n",
    "    create_file_for_final_window(issue_output_path, issues_iter)\n",
    "    create_file_for_final_window(pr_output_path, pr_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_input_issue_dataset_name = 'issues_last_window_sampled'\n",
    "sampled_input_pr_dataset = 'prs_last_window_sampled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Do the same thin again, but now randomly sample the data.\n",
    "t = 0.20\n",
    "\n",
    "\n",
    "def random_sampler(output_path:  str, data_iter: Iterator[Dict]):\n",
    "    sampled_count = 0\n",
    "    with open(output_path, 'w+', encoding='utf-8') as output_file:\n",
    "        for entry in data_iter:\n",
    "            if random.random() > t:\n",
    "                continue\n",
    "            sampled_count += 1\n",
    "            jdata = json.dumps(entry)\n",
    "            output_file.write(f'{jdata}\\n')\n",
    "    print(f'Sampled {sampled_count} entries.')\n",
    "\n",
    "\n",
    "sampled_issue_output_path = Path(exp_utils.CHRONOLOGICAL_DATASET_PATH(\n",
    "    file_name=sampled_input_issue_dataset_name, data_type='issues')).absolute()\n",
    "issues_iter = iterate_through_multiple_chronological_issue_pr_datasets([\n",
    "    str(Path(issue_output_path).with_suffix(\"\").name)], [])\n",
    "\n",
    "\n",
    "sampled_pr_output_path = Path(exp_utils.CHRONOLOGICAL_DATASET_PATH(\n",
    "    file_name=sampled_input_pr_dataset, data_type='pull-requests')).absolute()\n",
    "pr_iter = iterate_through_multiple_chronological_issue_pr_datasets([], [\n",
    "    str(Path(pr_output_path).with_suffix(\"\").name)])\n",
    "\n",
    "\n",
    "# This step can be skipped.\n",
    "sample_random = False\n",
    "\n",
    "if sample_random and sample_last_month:\n",
    "    random_sampler(sampled_issue_output_path, issues_iter)\n",
    "    random_sampler(sampled_pr_output_path, pr_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_issue_dataset_names = [sampled_input_issue_dataset_name]\n",
    "input_pr_dataset_names = [sampled_input_pr_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation with sliding window algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_proj.data_preprocessing.sliding_window_3 import create_sliding_window_dataset\n",
    "from python_proj.data_preprocessing.sliding_window_features.collaboration_experience.intra_eco_centrality import build_intra_eco_centrality_features\n",
    "import python_proj.data_preprocessing.sliding_window_features as swf\n",
    "from networkx import DiGraph\n",
    "from typing import Tuple, Set, List, Iterable\n",
    "from itertools import chain, product\n",
    "from wmutils.collections.safe_dict import SafeDict\n",
    "\n",
    "from python_proj.data_preprocessing.sliding_window_features.collaboration_experience.intra_eco_centrality import _build_edge_key\n",
    "from python_proj.data_preprocessing.sliding_window_features import (\n",
    "    SlidingWindowFeature,\n",
    "    Feature,\n",
    ")\n",
    "from python_proj.data_preprocessing.sliding_window_features.collaboration_experience.centrality_features import SNAFeature\n",
    "from python_proj.data_preprocessing.sliding_window_features.collaboration_experience.intra_eco_centrality import SNACentralityFeature\n",
    "import python_proj.utils.exp_utils as exp_utils\n",
    "\n",
    "\n",
    "class NormalizedSecondOrderEcosystemDegreeCentrality(SNACentralityFeature):\n",
    "    \"\"\"\n",
    "    Note: this is not multi-layer degree centrality.\n",
    "    That can only be calculated after the fact as it\n",
    "    requires the total number of included edges for\n",
    "    each layer. Instead, we return the individual\n",
    "    values $d(x, p, \\lambda, \\mu)$.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, graph: DiGraph, edge_to_project_mapping: dict, edge_types: list[SNAFeature]) -> None:\n",
    "        super().__init__(graph)\n",
    "        self._edge_to_project_mapping: dict = edge_to_project_mapping\n",
    "        self._edge_types = edge_types\n",
    "\n",
    "    def add_entry(self, *args, **kwargs):\n",
    "        # This class doesn't process any data entries itself, it relies on the entries of `self._edge_types`.\n",
    "        return\n",
    "\n",
    "    def remove_entry(self, *args, **kwargs):\n",
    "        # This class doesn't process any data entries itself, it relies on the entries of `self._edge_types`.\n",
    "        return\n",
    "\n",
    "    def get_name(self) -> Iterator[str]:\n",
    "        base_name = super().get_name()\n",
    "        for la, mu in product(self._edge_types, self._edge_types):\n",
    "            name = f'{base_name}({la.get_name()}.{mu.get_name()})'\n",
    "            yield name\n",
    "\n",
    "    def get_feature(self, entry: dict) -> list[int]:\n",
    "        x = entry[\"user_data\"][\"id\"]\n",
    "        repo_path = entry[exp_utils.SOURCE_PATH_KEY]\n",
    "        p = exp_utils.get_repository_name_from_source_path(repo_path)\n",
    "\n",
    "        card_R_la, d_la_mu = self.calculate_d_la_mu(x, p)\n",
    "\n",
    "        results = self.generate_output_vector(card_R_la, d_la_mu)\n",
    "        return results\n",
    "\n",
    "    def calculate_d_la_mu(self, x: int, p: str) -> Tuple[Dict[str, int], Dict[str, Dict[str, int]]]:\n",
    "        card_R_la = SafeDict(default_value=0)\n",
    "        d_la_mu = SafeDict(SafeDict, default_value_constructor_kwargs={\n",
    "            'default_value': 0.0})\n",
    "\n",
    "        # Collects the neighbors from incoming and outgoing edges; i.e., R.\n",
    "        R = self.get_neighbors(x)\n",
    "        for y in R:\n",
    "            E_xy = self.get_edges_between(x, y)\n",
    "\n",
    "            # Iterates through all edges that connect with the neighbor;\n",
    "            # This is stil the outer loop.\n",
    "            for la, T_xy in E_xy:\n",
    "                for e_t in T_xy:\n",
    "                    # Filters out edges in the same project.\n",
    "                    q_key = _build_edge_key(y, x, e_t, la)\n",
    "                    if q_key in self._edge_to_project_mapping:\n",
    "                        q = self._edge_to_project_mapping[q_key]\n",
    "                    else:\n",
    "                        q_key_2 = _build_edge_key(x, y, e_t, la)\n",
    "                        q = self._edge_to_project_mapping[q_key_2]\n",
    "                    if p == q:\n",
    "                        break\n",
    "\n",
    "                    # Any element that reaches beyond this point is an element of R_\\lambda(x, p).\n",
    "\n",
    "                    # Updates the counter, used to normalize the entry ($1/|R_\\lambda(x, p)|$).\n",
    "                    card_R_la[la] += 1\n",
    "                    nb_degree = self.calculate_nb_degree(x, y, e_t)\n",
    "                    for mu, d in nb_degree.items():\n",
    "                        d_la_mu[la][mu] += d\n",
    "\n",
    "        return card_R_la, d_la_mu\n",
    "\n",
    "    def calculate_nb_degree(self, x: int,  y: int, t: float) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Calculates $e_\\mu = (|E_\\mu(y)| - |E_\\mu(y, x)|)$ differentiating\n",
    "        between each layer of the graph, for a single neighbor $y$.\n",
    "        \"\"\"\n",
    "\n",
    "        e_mu = SafeDict(default_value=0)\n",
    "        # Collects the neighbors from incoming and outgoing edges; i.e., R.\n",
    "        # and removes all entries that are x.\n",
    "        E = self.get_neighbors(y)\n",
    "        E = (z for z in E if z != x)\n",
    "        for z in E:\n",
    "            E_yz = self.get_edges_between(y, z)\n",
    "            # Per edge, it retrieves the timed edges, and iterates through those.\n",
    "            for mu, T_yz in E_yz:\n",
    "                for e_t in T_yz:\n",
    "                    # Ignores non-chronological edges.\n",
    "                    if e_t >= t:\n",
    "                        continue\n",
    "                    e_mu[mu] += 1\n",
    "        return e_mu\n",
    "\n",
    "    def get_neighbors(self, x: int) -> Set[int]:\n",
    "        # Collects the neighbors from incoming and outgoing edges; i.e., R.\n",
    "        # The first element of the in_edge_tuple is the neighbor.\n",
    "        e_in = self._graph.in_edges(nbunch=[x])\n",
    "        e_in = ((v, u) for u, v in e_in)\n",
    "        # The second element of the out_edge tuple is the neighbor.\n",
    "        e_out = self._graph.out_edges(nbunch=[x])\n",
    "        nbs = {neighbor_id for _, neighbor_id in chain(e_in, e_out)}\n",
    "        return nbs\n",
    "\n",
    "    def get_edges_between(self, x: int, y: int) -> List[Dict]:\n",
    "        def __helper_get_edges_between() -> Iterable[Tuple[str, Iterable[float]]]:\n",
    "            \"\"\"Helper method to iterate through all edges connnecting x and y.\"\"\"\n",
    "            E_in = self._graph.get_edge_data(x, y)\n",
    "            if not E_in is None:\n",
    "                yield from E_in.items()\n",
    "            E_out = self._graph.get_edge_data(y, x)\n",
    "            if not E_out is None:\n",
    "                yield from E_out.items()\n",
    "\n",
    "        def __helper_filter_duplicates(edges) -> Iterable[Tuple[str, Iterable[float]]]:\n",
    "            # NOTE: The implementation of SNAFeature, which this class relies on does not consider duplicate edges.\n",
    "            # However, these do affect the calculations, so need to be accounted for.\n",
    "            # Characteristics of a duplicate edge are: same source and target node, same time stamp, same layer.\n",
    "            # Within the context of this method, this comes down to testing their timestamps.\n",
    "            for layer, edges in edges:\n",
    "                exclusion_list = set()\n",
    "                unique_edges = []\n",
    "                for edge in edges:\n",
    "                    if edge in exclusion_list:\n",
    "                        continue\n",
    "                    exclusion_list.add(edge)\n",
    "                    unique_edges.append(edge)\n",
    "                yield layer, unique_edges\n",
    "\n",
    "        edges = __helper_get_edges_between()\n",
    "        edges = __helper_filter_duplicates(edges)\n",
    "        edges = list(edges)\n",
    "        return edges\n",
    "\n",
    "    def generate_output_vector(self, card_R_la: Dict[str, int], d_la_mu: Dict[str, Dict[str, int]]) -> Dict[str, Dict[str, float]]:\n",
    "        # Normalizes the data and adds it to an output vector.\n",
    "        out = []\n",
    "        for la, mu in product(self._edge_types, self._edge_types):\n",
    "            l = la.get_name()\n",
    "            m = mu.get_name()\n",
    "            norm_d_la_mu = d_la_mu[l][m] / (1 + card_R_la[l])\n",
    "            out.append(norm_d_la_mu)\n",
    "            if norm_d_la_mu > 0:\n",
    "                pass\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following calculates $d(x, p, \\lambda, \\mu)$ for each tuple $\\lambda, \\mu$ and outputs those scores to the output file.\n",
    "These sub-features need to be aggregated into $S(x, p)$ at a later stage in this pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implements the feature factory for the sliding window.\n",
    "It loads the features necessary to populate the multilayer graph, the feature implemented above, and general ecosystem contribution features.\n",
    "This last one is added so we can test correlation later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_factory() -> Tuple[list[SlidingWindowFeature], list[SlidingWindowFeature], list[Feature]]:\n",
    "    pr_graph_features, issue_graph_features, _ = build_intra_eco_centrality_features()\n",
    "    graph = pr_graph_features[0]._graph\n",
    "    edge_to_project_mapping = pr_graph_features[0]._edge_to_project_mapping\n",
    "\n",
    "    edge_types = list(chain(pr_graph_features, issue_graph_features))\n",
    "\n",
    "    pr_graph_features.append(\n",
    "        NormalizedSecondOrderEcosystemDegreeCentrality(graph, edge_to_project_mapping, edge_types))\n",
    "\n",
    "    other_pr = swf.build_other_features()\n",
    "    control_sw, control = swf.build_control_variables()\n",
    "    ip_issue, ip_pr = swf.build_intra_project_features()\n",
    "    eco_se_pr, eco_se_issue = swf.build_eco_se_features()\n",
    "    eco_pr, eco_issue = swf.build_eco_experience()\n",
    "    deco_pr, deco_issue, ideco_pr, ideco_issue = swf.build_deco_features()\n",
    "\n",
    "    issue_sw_features = [\n",
    "        *ip_issue,\n",
    "        *eco_se_issue,\n",
    "        *eco_issue,\n",
    "        *deco_issue,\n",
    "        *ideco_issue,\n",
    "        *issue_graph_features,\n",
    "    ]\n",
    "\n",
    "    pr_sw_features = [\n",
    "        *control_sw,\n",
    "        *ip_pr,\n",
    "        *eco_se_pr,\n",
    "        *eco_pr,\n",
    "        *deco_pr,\n",
    "        *ideco_pr,\n",
    "        *pr_graph_features,\n",
    "    ]\n",
    "\n",
    "    pr_features = [\n",
    "        *other_pr,\n",
    "        *control,\n",
    "    ]\n",
    "\n",
    "    # Returns a triple: issue_sw_features, pr_sw_features, pr_features\n",
    "    return issue_sw_features, pr_sw_features, pr_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates the to-be-evaluated dataset dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_all_data = False\n",
    "\n",
    "if analyze_all_data:\n",
    "    print(\"Running with whole dataset.\")\n",
    "    input_issue_dataset_names = [issue_dataset]\n",
    "    input_pr_dataset_names = [pr_dataset]\n",
    "    thread_count = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output path: \"/workspaces/msc_thesis/python_proj/modelling/notebooks/single_model_experimentation/data/libraries/output/npm-libraries-1.6.0-2020-01-12/non_ftc_data.csv\".\n",
      "Total SNAFeature edge counts:\n",
      "null\n"
     ]
    }
   ],
   "source": [
    "# This is a debug option. This should be false.\n",
    "only_aggregate = True\n",
    "\n",
    "if only_aggregate:\n",
    "    from python_proj.data_preprocessing.sliding_window_3 import __merge_chunk_results\n",
    "\n",
    "    chunk_file_names = [chunk_output_base_path.join(\n",
    "        f'{chunk_no}.') for chunk_no in range(thread_count)]\n",
    "\n",
    "    output_features: Iterable[Feature] = chain(*feature_factory())\n",
    "\n",
    "    output_features = [\n",
    "        feat for feat in output_features if feat.is_output_feature()]\n",
    "\n",
    "    __merge_chunk_results(output_path, chunk_file_names,\n",
    "                          chunk_output_base_path, output_features, delete_chunk=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not only_aggregate:\n",
    "    create_sliding_window_dataset(output_path, chunk_base_path, chunk_output_base_path,\n",
    "                                input_issue_dataset_names, input_pr_dataset_names,\n",
    "                                feature_factory, window_size_in_days,\n",
    "                                thread_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis setup\n",
    "\n",
    "Loads the generated data file and tests for correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_path = r'/workspaces/msc_thesis/python_proj/modelling/notebooks/single_model_experimentation/data/libraries/npm-libraries-1.6.0-2020-01-12'\n",
    "\n",
    "df: pd.DataFrame = pd.read_csv(input_path, header=0)\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df.columns))\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature aggregation\n",
    "\n",
    "Aggregates the 25 sub-metrics $d(x, p, \\lambda, \\mu)$ into $S(x, p)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is copied from one of the output cells.\n",
    "edge_counts = {\n",
    "    \"IssueCommenterToCommenterV2\": 80400,\n",
    "    \"IssueCommenterToSubmitterV2\": 9994,\n",
    "    \"PRIntegratorToSubmitterV2\": 2150,\n",
    "    \"PRCommenterToCommenterV2\": 41258,\n",
    "    \"PRCommenterToSubmitterV2\": 6263\n",
    "}\n",
    "total_edges = sum(edge_counts.values())\n",
    "\n",
    "\n",
    "def calculate_weight(value):\n",
    "    \"\"\"Calculates normalized complement of the weight.\"\"\"\n",
    "    percentage_complement = 1 - (value / (total_edges))\n",
    "    normalized_perc_compl = percentage_complement / (len(edge_counts) - 1)\n",
    "    return normalized_perc_compl\n",
    "\n",
    "\n",
    "edge_weights = {key: calculate_weight(value)\n",
    "                for key, value in edge_counts.items()}\n",
    "total_weight = sum(edge_weights.values())\n",
    "edge_weights = {key: value / total_weight for key,\n",
    "                value in edge_weights.items()}\n",
    "\n",
    "print(\"Edge weights =\", json.dumps(edge_weights, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "scopes = [\"Ecosystem\", \"IntraProject\"]\n",
    "edge_types = edge_weights.keys()\n",
    "directions = [\"In\", \"Out\"]\n",
    "combinations = product(edge_types, edge_types)\n",
    "\n",
    "BASE_EDGE_FIELD = \"NormalizedSecondOrderEcosystemDegreeCentrality({connecting_edge}.{experience_edge})\"\n",
    "TARGET_FIELD = \"NormalizedSecondOrderEcosystemDegreeCentrality\"\n",
    "\n",
    "centrality_features = []\n",
    "\n",
    "# Iterates through all edge variables.\n",
    "for connecting_edge, experience_edge in combinations:\n",
    "    edge_field = BASE_EDGE_FIELD.format(\n",
    "        connecting_edge=connecting_edge,\n",
    "        experience_edge=experience_edge,\n",
    "    )\n",
    "\n",
    "    # Applies the weights to edge.\n",
    "    connecting_weight = edge_weights[connecting_edge]\n",
    "    experience_weight = edge_weights[experience_edge]\n",
    "    total_weight = connecting_weight * experience_weight\n",
    "\n",
    "    df[edge_field] = df[edge_field].multiply(total_weight)\n",
    "\n",
    "    # Add contribution to centrality.\n",
    "    # target_field = BASE_TARGET_FIELD.format(scope=scope, direction=direction)\n",
    "    # Sets initial value if it doesn't exist yet.\n",
    "    if TARGET_FIELD not in df.columns:\n",
    "        df[TARGET_FIELD] = 0\n",
    "        centrality_features.append(TARGET_FIELD)\n",
    "\n",
    "    df[TARGET_FIELD] = df[TARGET_FIELD].add(df[edge_field])\n",
    "\n",
    "    # Drops the edge column, as it's only necessary once.\n",
    "    df = df.drop(edge_field, axis=1)\n",
    "\n",
    "print(f\"{len(df.columns)=}\")\n",
    "df[centrality_features].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming and scaling\n",
    "\n",
    "Applies add-one log transform to the data, removes outliers using cook's distance, and then applies min-max scaling to the leftover data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies add-one log transform\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "ln_key = \"ln(1 + {field})\"\n",
    "\n",
    "df: pd.DataFrame = df\n",
    "\n",
    "num_fields = df[df.columns[5:]].select_dtypes(include='number').columns\n",
    "\n",
    "for column in num_fields:\n",
    "    new_field = ln_key.format(field=column)\n",
    "    df[new_field] = df[column].add(1).apply(np.log)\n",
    "\n",
    "df = df.drop(num_fields, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies min-max scale\n",
    "\n",
    "def scale(_df: pd.DataFrame, scaled_fields: pd.Series):\n",
    "    scaled_df = _df.copy()\n",
    "\n",
    "    for feature in scaled_fields:\n",
    "        feature_min = scaled_df[feature].min()\n",
    "        feature_max = scaled_df[feature].max()\n",
    "        feature_delta = feature_max - feature_min\n",
    "\n",
    "        scaled_df[feature] = (\n",
    "            scaled_df[feature].subtract(feature_min).divide(feature_delta)\n",
    "        )\n",
    "\n",
    "    return scaled_df\n",
    "\n",
    "num_fields = df[df.columns[5:]].select_dtypes(include='number').columns\n",
    "df = scale(df, num_fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation testing\n",
    "\n",
    "Uses VIF and Spearman correlation to identify problematically multicollinear features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance inflation factor (VIF)\n",
    "\n",
    "Calculates variance inflation factor to identify problematic multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.columns[6:]]\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaces binary values with numeric variants so VIF can be calculated.\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "value_mapper = {False: 0, True: 1}\n",
    "\n",
    "binary_fields = df.select_dtypes(exclude=\"number\")\n",
    "print(f\"{len(binary_fields.columns)=}\")\n",
    "for feature in binary_fields:\n",
    "    df[feature] = df[feature].replace(value_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates VIF\n",
    "\n",
    "vif_df = pd.DataFrame()\n",
    "vif_df[\"variable\"] = df.columns\n",
    "vif_df[\"VIF\"] = [\n",
    "    variance_inflation_factor(df.values, i) for i in range(df.shape[1])\n",
    "]\n",
    "\n",
    "print(vif_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifies problematic fields, according to VIF.\n",
    "\n",
    "VIF_THRESHOLD = 5\n",
    "\n",
    "problematic_vif = vif_df[vif_df[\"VIF\"] >= VIF_THRESHOLD]\n",
    "\n",
    "print(f\"{VIF_THRESHOLD}\")\n",
    "print(f\"Problematic VIF fields: {len(problematic_vif)}.\")\n",
    "print(\"Problematic VIF scores:\\n\", problematic_vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spearman correlation\n",
    "\n",
    "Calculates pairwise spearman correlation between variables to make it easier to identify what variables are multicollinear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates Spearman correlation between variables.\n",
    "\n",
    "import json\n",
    "\n",
    "corr = df.corr(method=\"spearman\")\n",
    "coof_threshold = 0.0\n",
    "print(f\"{coof_threshold=}\")\n",
    "\n",
    "correlating_fields = []\n",
    "for rowIndex, row in corr.iterrows():\n",
    "    for columnIndex, coof in row.items():\n",
    "        if columnIndex <= rowIndex:\n",
    "            continue\n",
    "        if abs(coof) < coof_threshold:\n",
    "            continue\n",
    "        correlating_fields.append([columnIndex, rowIndex, coof])\n",
    "correlating_fields = [list(entry) for entry in correlating_fields]\n",
    "\n",
    "print(\"Correlating pairs:\\n\", json.dumps(correlating_fields, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates transitive closure of correlating fields;\n",
    "# i.e., it identifies clusters of correlating features.\n",
    "\n",
    "from typing import List, Set\n",
    "\n",
    "transitive_closures: List[Set[str]] = []\n",
    "\n",
    "# Iterates through all correlating fields.\n",
    "for field_a, field_b, _ in correlating_fields:\n",
    "    was_added = False\n",
    "    # Sees if one of the components is part of a transitive closure.\n",
    "    for transitive_closure in transitive_closures:\n",
    "        if field_a in transitive_closure or field_b in transitive_closure:\n",
    "            was_added = True\n",
    "            transitive_closure.update((field_a, field_b))\n",
    "    # Adds new closure if they are not.\n",
    "    if not was_added:\n",
    "        new_set = set()\n",
    "        new_set.update((field_a, field_b))\n",
    "        transitive_closures.append(new_set)\n",
    "\n",
    "# Prepares the closures to be printed.\n",
    "transitive_closures = [sorted(list(closure)) for closure in transitive_closures]\n",
    "\n",
    "print(\n",
    "    \"Transitive closure of correlating pairs:\\n\",\n",
    "    json.dumps(transitive_closures, indent=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots correlation as seaborn plot.\n",
    "\n",
    "import seaborn as sb\n",
    "\n",
    "tick_labels = list(range(len(vif_df)))\n",
    "rows, cols = corr.shape\n",
    "mask = np.invert(np.tril(np.ones((rows, cols), dtype=bool), k=-1))\n",
    "dataplot = sb.heatmap(\n",
    "    corr,\n",
    "    cmap=\"PuOr\",\n",
    "    xticklabels=tick_labels,\n",
    "    yticklabels=tick_labels,\n",
    "    mask=mask,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots correlations as a graph.\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "fields = list(df.columns)\n",
    "enumerated_list = list(enumerate(fields))\n",
    "ids = []\n",
    "edge_colors = []\n",
    "for source, target, coof in correlating_fields:\n",
    "    source_idx = fields.index(source)\n",
    "    target_idx = fields.index(target)\n",
    "    ids.append((source_idx, target_idx))\n",
    "    edge_colors.append(\"red\" if coof < 0 else \"green\")\n",
    "\n",
    "# Create an empty graph\n",
    "graph = nx.Graph()\n",
    "\n",
    "# Add edges to the graph from the list of tuples\n",
    "graph.add_edges_from(ids)\n",
    "\n",
    "pos = nx.spring_layout(graph, k=0.4)\n",
    "nx.draw(graph, pos, with_labels=True, edge_color=edge_colors)\n",
    "\n",
    "if len(correlating_fields) == 0:\n",
    "    print(\"When there are no correlating fields, the figure is supposed to be empty.\")\n",
    "\n",
    "print(json.dumps(enumerated_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing multicollinear variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
