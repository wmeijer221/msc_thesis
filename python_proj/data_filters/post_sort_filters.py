"""
Filters datasets based on various criteria (e.g. whether they're bots) 
using the sorted version of the dataset; i.e., the output of the 
``data_sorter`` scriptnot the loose files generated by the retrieval scripts.

Commandline arguments:
-e: the used ecosystem (npm)
-d: the used datasource (pull-requests)
-i: the used input file (sorted)
-o: the used output file (sorted_filtered)
"""


from python_proj.data_retrieval.retrieve_pull_requests import end_date
import json
from datetime import datetime
import re
from csv import reader

from python_proj.utils.arg_utils import get_argv
from python_proj.utils.exp_utils import build_data_path_from_argv, BASE_PATH


def load_data(input_path: str):
    data = []
    with open(input_path, "r") as input_data:
        for line in input_data:
            line = line.strip()
            j_entry = json.loads(line)
            data.append(j_entry)
    return data


def build_filters(selected_filter_keys: str):
    """Builds a filter for a series of keys."""
    # Standard filter: pcuadgbn
    all_filters = {
        'p': filter_for_github,
        'c': filter_by_close_date,
        'u': filter_for_bots_by_user_type,
        'a': filter_for_deleted_accounts,
        'd': filter_bots_dey_2020,
        'g': filter_bots_golzadeh_2021,
        'b': filter_for_blacklist,  # Add self-identified bots to this list.
        'n': filter_for_bot_in_name,
    }
    selected_filters = [all_filters[key] for key in selected_filter_keys]
    return selected_filters


def filter_by_close_date(entry):
    closed_at_key = 'closed_at'
    if not closed_at_key in entry:
        return False
    closed_date = entry[closed_at_key]
    closed_at_dt = datetime.strptime(closed_date, "%Y-%m-%dT%H:%M:%SZ")
    return closed_at_dt <= end_date


def filter_for_github(entry):
    created_at_key = 'created_at'
    created_date = entry[created_at_key]
    # HACK: GitLab uses a different time format. I've got no better way to test for GH PRs, though.
    try:
        datetime.strptime(created_date, "%Y-%m-%dT%H:%M:%SZ")
        return True
    except:
        return False


def filter_for_bots_by_user_type(entry):
    user_data = entry['user_data']
    user_type = user_data['type']
    return user_type.lower() != 'bot'


dey_bots_names = None
dey_bots_emails = None


def filter_bots_dey_2020(entry):
    global dey_bots_emails, dey_bots_names

    # HACK: make-shift init function for the filter.
    if dey_bots_emails is None or dey_bots_emails is None:
        filter_path = BASE_PATH + "bot_data/dey_2020_bots.json"
        with open(filter_path, "r") as input_file:
            j_data = json.loads(input_file.read())
            dey_bots_emails = set()
            dey_bots_names = set()
            for bot_entry in j_data:
                dey_bots_emails.add(bot_entry['email'])
                dey_bots_names.add(bot_entry['name'].lower())

    # Actual filter
    user_data = entry["user_data"]
    if ("email" in user_data and user_data["email"] in dey_bots_emails) \
        or user_data["login"].lower() in dey_bots_names \
            or ('name' in user_data and user_data["name"].lower() in dey_bots_names):
        return False

    return True


golzadeh_bots_names = None


def filter_bots_golzadeh_2021(entry):
    global golzadeh_bots_names

    # HACK: make-shift init function.
    if golzadeh_bots_names is None:
        filter_path = BASE_PATH + "bot_data/golzadeh_2021_bots.csv"
        with open(filter_path, "r") as filter_file:
            filter_reader = reader(filter_file, quotechar='"')
            _ = next(filter_reader)  # Skips header
            golzadeh_bots_names = {entry[0].lower() for entry in filter_reader}

    # actual filter
    user_data = entry["user_data"]
    if user_data["login"].lower() in golzadeh_bots_names:
        return False

    return True


def filter_for_deleted_accounts(entry):
    user_data = entry['user_data']
    user_login = user_data['login'].lower()
    # Accounts that are deleted are represented by the 'ghost' account. See https://github.com/ghost
    return user_login != 'ghost'


def filter_for_blacklist(entry):
    user_data = entry['user_data']
    user_login = user_data['login'].lower()
    return not user_login in ["fabric8cd", "mrsflux", "greenkeeperio-bot", "ovh-ux-cds"]


def filter_for_bot_in_name(entry):
    user_data = entry['user_data']
    login = user_data['login']
    for _ in re.finditer('[bot]', login):
        return True
    return False


def filter_data(original_data: list, filter_methods: list) -> list:
    """Iterates through data and applies provided filters."""
    filtered = []
    filtered_by = [0] * len(filter_methods)
    for entry in original_data:
        is_included = True
        for index, filter_method in enumerate(filter_methods):
            try:
                if not filter_method(entry):
                    filtered_by[index] += 1
                    is_included = False
                    break
            except:
                print(f'Failed with {entry=} and {filter_method=}.')
                raise
        if is_included:
            filtered.append(entry)
    print(filtered_by)
    return filtered


def write_data(data: list, output_path: str):
    with open(output_path, "w+", encoding='utf-8') as output_file:
        for entry in data:
            output_file.write(f'{json.dumps(entry)}\n')


if __name__ == "__main__":
    # TODO: replace this to use ``exp_utils.load_argv()``.
    input_path = build_data_path_from_argv(file_name_key='-i')
    output_path = build_data_path_from_argv(file_name_key='-o')
    mode = get_argv("-m")

    filters = build_filters(mode)
    data = load_data(input_path)
    print(f'Start size: {len(data)}.')

    filtered_data = filter_data(data, filters)
    print(f'Filtered size: {len(filtered_data)}.')
    write_data(filtered_data, output_path)
